{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if running for the first time)\n",
    "!pip install transformers torch bitsandbytes kagglehub peft\n",
    "\n",
    "# Import necessary libraries\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Set device for model computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to download and load the COVID-19 Symptoms dataset\n",
    "def load_covid_symptoms_dataset():\n",
    "    path_symptoms = kagglehub.dataset_download(\"takbiralam/covid19-symptoms-dataset\")\n",
    "    symptoms_path = f\"{path_symptoms}/covid-19 symptoms dataset.csv\"\n",
    "    symptoms_df = pd.read_csv(symptoms_path)\n",
    "    return symptoms_df\n",
    "\n",
    "# Function to map and preprocess the dataset for readability\n",
    "def preprocess_data(symptoms_df):\n",
    "    diff_breath_map = {-1: \"no difficulty breathing\", 0: \"moderate difficulty breathing\", 1: \"severe difficulty breathing\"}\n",
    "    binary_map = {0: \"no\", 1: \"yes\"}\n",
    "    symptoms_df['bodyPain'] = symptoms_df['bodyPain'].map(binary_map)\n",
    "    symptoms_df['runnyNose'] = symptoms_df['runnyNose'].map(binary_map)\n",
    "    symptoms_df['diffBreath'] = symptoms_df['diffBreath'].map(diff_breath_map)\n",
    "    symptoms_df['infectionProb'] = symptoms_df['infectionProb'].map(binary_map)\n",
    "    return symptoms_df\n",
    "\n",
    "# Function to generate fine-tuning prompts and responses from dataset\n",
    "def generate_fine_tune_data(symptoms_df):\n",
    "    data = [\n",
    "        {\n",
    "            \"prompt\": (f\"Patient has fever: {row['fever']}Â°F, body pain: {row['bodyPain']}, \"\n",
    "                       f\"age: {row['age']}, runny nose: {row['runnyNose']}, difficulty breathing: {row['diffBreath']}.\"),\n",
    "            \"response\": \"High\" if row['infectionProb'] == \"yes\" else \"Low\"\n",
    "        }\n",
    "        for _, row in symptoms_df.iterrows()\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Hugging Face login (replace with your token)\n",
    "def authenticate_hugging_face(token):\n",
    "    login(token)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "def initialize_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "    # Ensure pad token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "# Tokenize input data\n",
    "def tokenize_data(fine_tune_data, tokenizer):\n",
    "    def tokenize_function(example):\n",
    "        input_text = f\"<s>Prompt: {example['prompt']}\\nResponse: {example['response']}</s>\"\n",
    "        return tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    return fine_tune_data.apply(tokenize_function, axis=1)\n",
    "\n",
    "# PyTorch Dataset class for fine-tuning\n",
    "class FineTuneDataset(Dataset):\n",
    "    def __init__(self, inputs):\n",
    "        self.input_ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.input_ids[idx]\n",
    "        }\n",
    "\n",
    "# Prepare dataset and tokenize in batch mode\n",
    "def prepare_dataset(fine_tune_data, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        fine_tune_data['prompt'].tolist(),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    return FineTuneDataset(inputs)\n",
    "\n",
    "# Configure PEFT (LoRA) settings\n",
    "def configure_peft(model):\n",
    "    peft_config = LoraConfig(\n",
    "        r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.1, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    return PeftModel(model, peft_config)\n",
    "\n",
    "# Set training arguments\n",
    "def get_training_args():\n",
    "    return TrainingArguments(\n",
    "        output_dir=\"./finetuned-covid\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "# Train the model using Hugging Face Trainer\n",
    "def train_model(model, train_dataset):\n",
    "    training_args = get_training_args()\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained(\"./finetuned-covid\")\n",
    "    tokenizer.save_pretrained(\"./finetuned-covid\")\n",
    "    print(\"Fine-tuning with PEFT completed and models saved.\")\n",
    "\n",
    "# Generate responses from fine-tuned model\n",
    "def generate_response(prompt, model, tokenizer):\n",
    "    inputs = tokenizer(f\"<s>Prompt: {prompt}</s>\", return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Main workflow\n",
    "def main():\n",
    "    # Load and preprocess dataset\n",
    "    symptoms_df = load_covid_symptoms_dataset()\n",
    "    symptoms_df = preprocess_data(symptoms_df)\n",
    "\n",
    "    # Generate fine-tuning data\n",
    "    fine_tune_data = generate_fine_tune_data(symptoms_df)\n",
    "\n",
    "    # Authenticate and initialize model\n",
    "    authenticate_hugging_face(\"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\")  # Replace with your actual token\n",
    "    tokenizer, model = initialize_model_and_tokenizer(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "    # Prepare dataset for fine-tuning\n",
    "    tokenized_data = tokenize_data(fine_tune_data, tokenizer)\n",
    "    train_dataset = prepare_dataset(fine_tune_data, tokenizer)\n",
    "\n",
    "    # Configure PEFT for the model and start training\n",
    "    model = configure_peft(model)\n",
    "    train_model(model, train_dataset)\n",
    "\n",
    "    # Example test\n",
    "    test_prompt = \"Patient symptoms: fever, dry cough, and fatigue. Is this likely COVID-19?\"\n",
    "    print(\"Response:\", generate_response(test_prompt, model, tokenizer))\n",
    "\n",
    "# Execute main workflow\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
